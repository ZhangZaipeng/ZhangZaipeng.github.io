一、RDD的基本概
    RDD是什么？
        RDD是弹性分布式数据集
        是Spark对数据的抽象
        Spark的本质就是对RDD的创建、转化、处理
    对RDD一般都执行哪些操作
        创建  转化  行动
        输入 --> RDD --> RDD --> 输出  （RDD与RDD之间进行转化）
    分区的定义：
        一个RDD有多个RDD分区
        一个RDD分区只在一个机器上
        一个机器可有多个RDD分区
二、如何创建RDD
    从集合创建 （它们分别在3个分区中。这个RDD一共9个元素，每个元素含有一个数字）
        scala> val input = sc.parallelize(1 to 9, 3)
        parallelCollectionRDD

    从文件创建 （文件中的每一行就是RDD中的一个元素，分区是2）
        本地 scala> val input = sc.textFile("file:// ... "，2)
        MapPartitionRDD

        HDFS scala> val input = sc.textFile("hdfs://hostname:9000/filename")
        MapPartitionRDD
    作业： 大小文件加载速度？是否加载成功？

三、RDD支持的两类操作
    1.RDD的转化操作(没有真正执行)
        1.RDD filter()方法
        scala> val filter_a_RDD = input.filter(line => line.contains("a"))
        scala> val filter_b_RDD = input.filter(line => line.contains("b"))
        scala> val filter_d_RDD = filter_a_RDD.filter(line => line.contains("d"))
        注：
            RDD转化操作是惰性计算
            只在必要的时候读取数据，避免交互大量的数据
            更容易从结果逆向构建计算过程

        2.针对单个RDD常用的转化操作
            def filter(f:(T) => Boolean): RDD[T]
                过滤包含关键字的行

            def first():T
                显示文件第一行

            def map[U](f:(T) => U)(implicit arg0:ClassTag[U]):RDD
                对RDD中的每个元素都执行一个指定的函数f来产生一个新的RDD。
                任何原RDD中的元素在新RDD中都有且只有一个元素与之对应。

                eg: val words = sc.parallelize( List("kgc", "kegongchang", "spark", "hello"))
                    val word_length = words.map(_.length)
                    word_length.collect()
                    res: Array[Int] = Array(3, 11, 5, 5)

            def zip[U](other:RDD[U])(implicit arg0:ClassTag[U]):RDD
                对两个RDD并行遍历
                eg: words.zip(word_length).collect()
                res: Array[(String,Int)] = Array((kgc,3),(kegongchang,11),(spark,5),(hello,5))

            def flatMap[U](f:(T) =>TraversableOnce[U])(implicit arg0:ClassTag[U]):RDD[U]
                映射成多个元素构建新的RDD
                map和flatmap区别在与：map处理完成生产一个元素，而flatmap可以生产多个元素构建新的RDD
                eg: val nums = sc.parallelize(1 to 4 , 2)
                    val num2 = nums.flatMap( x => 1 to x)
                    num2.collect()
                    res : Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4)

                    val lines =sc.parallelize( List("Hello Spark","kegongchang"))
                    lines.flatMap( line => line.split(" ") ).collect()
                    res: Array[String] = Array(Hello, Spark, kegongchang)

                    lines.map( line => line.split(" ") ).collect()
                    res: Array[Array[String]] = Array(Array(Hello, Spark), Array(kegongchang))

            def mapPartitions(function)
                map()的输入函数是应用于RDD中每个元素，而mapPartitions()的输入函数是应用于每个分区
                eg: val input = spark.parallelize(List(1, 2, 3, 4, 5, 6), 2)//RDD有6个元素，分成2个partition
                    val result = input.mapPartitions(
                          partition => Iterator(sumOfEveryPartition(partition))
                    )//partition是传入的参数，是个list，要求返回也是list，即Iterator(sumOfEveryPartition(partition))
                    result.collect().foreach {
                        println(_)//6 15
                    }
            def distinct():RDD[T]
                元素去重
        3.针对两个RDD常用的转化操作
            def union(other: RDD[T]): RDD[T]
                输出两个RDD的所用元素,不去掉重复元素
                eg: val num1 = sc.parallelize( List("aa", "bb", "cc", "cc"), 2 )
                    val num2 = sc.parallelize( List("cc", "dd"), 2 )
                    num1.union(num2).collect()
                    res: Array[String] = Array(aa, bb, cc, cc, cc, dd)

            def intersection(other: RDD[T], numPartition:Int): RDD[T]
                输出两个RDD相同的元素,去重
                eg: num1.intersection(num2).collect()
                res: Array[String] = Array(cc)

            def subtract(other: RDD[T], p:Partitioner)(implicit ord: Ordering[T] = null): RDD[T]
                返回包含在第一个RDD中但是不包含在第二个RDD中的元素，不去重
                eg: num1.subtract(num2).collect()
                res: Array[String] = Array(aa, bb)

            def cartesian[U](other: RDD[U]):(implicit arg0:ClassTag[U]):RDD[(T,U)]
                返回两个元素的笛卡尔积
                eg: num1.cartesian(num2).collect()
                res: Array[(String,String)] = Array((aa,cc),(bb,cc),(aa, dd),(bb,dd), ...)

    2.RDD的行动操作（开始运行）
        def collect():Array[T]
            返回全部元素

        def take(num:Int):Array[T]
            返回指定数量的元素
            eg: val lines = sc.textFile("file:// ... ")
                lines.take(2)
            res:Array[String] = Array("", "") 前两行数据

        def foreach(f:(T) => Unit):Unit
            应用方法到每一个元素
            eg: val lines = sc.textFile("file:// ... ")
                lines.take(2).foreach(println)
                res: 输出每一行数据

        def count()：Long
            返回RDD的元素个数

        def countByValue(implicit ord:Ordering[T] = null):Map[T,Long]
            统计每一个元素数量

            count和countByValue的区别在于：
                count返回总数，countByValue统计每个元素的数量返回一对键值对

        def reduce(f:(T,T) => T):T
            对每一个元素进行逐个合并
            reduce将RDD中元素前两个传给输入函数，产生一个新的return值，
            新产生的return值与RDD中下一个元素（第三个元素）组成两个元素，再被传给输入函数，直到最后只有一个值为止。
            eg: val num = sc.parallelize(1 to 10 ,2)
                num.reduce((x,y) => x + y)
                res Int = 55
    3.RDD的RDD转化谱系图
        见图

大白话理解RDD
http://blog.csdn.net/guotong1988/article/details/50556871